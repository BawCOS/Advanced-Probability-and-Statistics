---
title: "Lesson 24: Confidence Intervals"
author: "Lt Col Ken Horton"
date: "October 15, 2019"
header-includes: 
  - \usepackage{amsmath,multirow}
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align='center')
knitr::opts_chunk$set(out.width = "75%")
```

\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\Prob}{\mbox{P}}
\newcommand{\diff}{\,\mathrm{d}}


# Objectives

1) Define the term confidence interval. 

2) Using asymptotic methods, obtain and interpret a confidence interval for an unknown parameter, based on a random sample. 

3) Describe the relationships between confidence intervals, confidence level, and sample size. 


## Estimation Uncertainty

Earlier in this block, we covered estimation techniques for unknown parameters. In lesson 22, we learned that the sample mean is always a good estimator for the population mean, regardless of the underlying distribution. However, a random sample likely will not yield a sample mean that is *exactly* equal to the population mean. For example, suppose the mean student height at a local college is 67 inches. If I take a sample of 25 students, the mean of that sample will probably not be 67 inches exactly. It could be 67.5 inches. Or perhaps 66.1 inches. It could be 64 inches, although that would be much more unlikely. The point is, it is reasonable to expect that the mean of a representative sample should be *around* the population mean. 

In this lesson, we will define intervals that help specify exactly what *around* means. 

## Confidence Intervals

Generally, what you should know about building confidence intervals is laid out in the following steps:

1) Identify the parameter you would like to estimate (for example, $\mu$). 

2) Identify a good estimate for that parameter (sample mean, $\bar{X}$). 

3) Determine the distribution of your estimate or a function of your estimate. This tells us where our estimate *should* be if we knew the value of our parameter. (According to the central limit theorem, ${\bar{X}-\mu\over\sigma/\sqrt{n}}\sim \textsf{Norm}(0,1)$ and ${\bar{X}-\mu\over S/\sqrt{n}}\sim \textsf{t}(n-1)$). 

4) Use this distribution to obtain a range of feasible values (confidence interval) for the parameter. (For $\mu$, we can solve for $\mu$ to find a reasonable range; see below). 

In Lesson 25, we will discuss simulation methods for confidence intervals. The four steps above are the exact same. However, in step 3, simulation (rather than asymptotic theory) will be used to obtain the distribution of our estimate. 

### Confidence Intervals for Means

Let $X_1,X_2,...,X_n$ be an iid sequence of random variables, each with mean $\mu$ and standard deviation $\sigma$. The central limit theorem tells us that
$$
{\bar{X}-\mu\over \sigma/\sqrt{n}}\overset{approx}{\sim}\textsf{Norm}(0,1)
$$

Let $0\leq \alpha \leq 1$. Then,
$$
\Prob\left(-z_{\alpha/2}\leq {\bar{X}-\mu\over \sigma/\sqrt{n}} \leq z_{\alpha/2}\right)=1-\alpha
$$

where $z_{\alpha/2}$ is such that $\Prob(Z\geq z_{\alpha/2})=\alpha/2$, where $Z\sim \textsf{Norm}(0,1)$. A picture would help:
```{r lesson24a,echo=F}
curve(dnorm(x),from=-3.5,to=3.5,xaxt="n",xlab="",ylab="f")
polygon(c(-1.5,seq(-1.5,1.5,0.01),1.5),c(0,dnorm(seq(-1.5,1.5,0.01)),0),density=15)
text(-1.55,0.35,expression(paste("Prob=1-",alpha,sep="")))
arrows(-1.5,0.32,-0.8,0.25,length=0.1)
axis(1,at=c(-1.5,1.5),labels=c(expression(paste("-z_",alpha,"/2",sep="")),expression(paste("z_",alpha,"/2",sep=""))))
text(-3,0.08,expression(paste("Prob=",alpha,"/2",sep="")))
arrows(-3,0.06,-2.25,0.02,length=0.1)
text(3,0.08,expression(paste("Prob=",alpha,"/2",sep="")))
arrows(3,0.06,2.25,0.02,length=0.1)
```

So, I know that $(1-\alpha)*100\%$ of the time, ${\bar{X}-\mu\over \sigma/\sqrt{n}}$ will be between $-z_{\alpha/2}$ and $z_{\alpha/2}$. 

By rearranging the expression above and solving for $\mu$, we get:
$$
\Prob\left(\bar{X}-z_{\alpha/2}{\sigma\over\sqrt{n}}\leq \mu \leq \bar{X}+z_{\alpha/2}{\sigma\over\sqrt{n}}\right)=1-\alpha
$$

Be careful with the interpretation of this expression. As a reminder $\bar{X}$ is the random variable here. The population mean, $\mu$, is NOT a variable. It is an unknown parameter. Thus, the above expression is NOT a probabilistic statement about $\mu$, but rather about $\bar{X}$. 

Nonetheless, the above expression gives us a nice interval for "reasonable" values of $\mu$ given a particular sample. 

A $(1-\alpha)*100\%$ *confidence interval for the mean* is given by:
$$
\mu\in\left(\bar{X}\pm z_{\alpha/2}{\sigma\over\sqrt{n}}\right)
$$

When interpreting a confidence interval, it is important to remember what is random. Think of $\mu$ as a *fixed* value. It has a specific, unchanging value; but we don't know it. What is random is the sample. So if I take a random sample and construct a $(1-\alpha)*100\%$ confidence interval, then take another random sample of the same size, construct another interval, and repeat that process over and over, $(1-\alpha)*100\%$ of those intervals will contain that fixed, unknown value of $\mu$. 

In most applications, the most common value of $\alpha$ is 0.05. In that case, to construct a 95% confidence interval, we would need to find $z_{0.025}$ which can be found quickly with `qnorm()`:
```{r lesson24b}
qnorm(0.975)
```

#### Unknown Variance

As we discussed in lesson 23, when inferring about the population mean, we usually will have to estimate the underlying standard deviation as well. This introduces an extra level of uncertainty. We found that while ${\bar{X}-\mu\over\sigma/\sqrt{n}}$ has an approximate normal distribution, ${\bar{X}-\mu\over S/\sqrt{n}}$ follows the $t$-distribution with $n-1$ degrees of freedom. 

Thus, when $\sigma$ is unknown, a $(1-\alpha)*100\%$ confidence interval is given by: 
$$
\mu\in\left(\bar{X}\pm t_{\alpha/2,n-1}{s\over\sqrt{n}}\right)
$$

Similar to the case above, $t_{\alpha/2,n-1}$ can be found using the `qt()` function in `R`. 

### Example 24.1

Recall Example 23.1. We would like to estimate the mean height of students at a local college. We collect a sample of size 50 and find a sample mean of 68.7 and a sample standard deviation of 5.3. Find and interpret a 95% confidence interval for $\mu$. 

$$
\mu\in\left(\bar{X}\pm t_{\alpha/2,n-1}{s\over\sqrt{n}}\right)
$$
```{r lesson24c}
xbar<-68.7
s<-5.3
n<-50
tval<-qt(0.975,49)
xbar+c(-1,1)*tval*s/sqrt(n)
```

The 95% confidence interval for $\mu$ is $(67.19,70.21)$. I am 95% *confident* that $\mu$ is in this interval. Also, we could say that 95% of similarly constructed intervals will contain the true mean, $\mu$. 

### Confidence Interval for Difference in Means

Suppose we were interested in comparing two populations. For example, we may have a theory that students at our local college are taller than students at the other nearby college, on average. We could build a confidence interval on $\mu_1-\mu_2$ (the difference between the two population means) by comparing samples from the two populations. 

I will not go into the specifics, but there are various methods to build these confidence intervals, depending on whether the populations are assumed to have equal or unequal variances. For more information, I recommend consulting IPSUR or any number of online resources. For this scenario, I would recommend simulation methods. 

### Confidence Interval for Proportions

In lesson 23, we demonstrated the central limit theorem with a example involving binary data. Recall Example 23.2. In this example, there is an upcoming election in Colorado and Proposition A is on the ballot. Now suppose that we do not know $\pi$ the true proportion of Colorado voters that support the measure. So, we will take a representative sample (a poll). Our best guess for $\pi$ is the proportion of our sample that supports the measure (or $\hat{\pi}$). 

Next, we need to figure out the distribution of our estimate. In Example 23.2, we demonstrated that $\hat{\pi}$ (we called it $\bar{X}$ in the example) had an approximate normal distribution, thanks to the central limit theorem. Thus, we can use this to construct a confidence interval on $\pi$. However, that is just one way to construct an interval for $\pi$. We will explore this methodology in the practical application. 

Instead of using the distribution of $\hat{\pi}$, let's find the distribution of $Y=n\hat{\pi}$, or the number of people in the sample who expressed support for the measure. Assuming respondents in the sample are independent of one another, we know that $Y\sim\textsf{Binom}(n,\pi)$. We can use this to find a range of feasible values for $\pi$. 

### Example 24.2

Assume we took a representative sample of 50 Colorado voters and 28 expressed support for the measure. Find a confidence interval for $\pi$. 

We need $\pi_L$ and $\pi_U$ such that $\Prob(Y\geq 28|\pi=\pi_L) \leq  0.025$ and $\Prob(Y\leq 28|\pi=\pi_U)\leq 0.025$. There's not a simple function that does this, so we need to explore a grid of possible values of $\pi$ and find the appropriate $\pi_L$ and $\pi_U$:

```{r lesson24d}
#Create a grid of possible values of pi
pis<-seq(0.2,0.8,0.001)

#Upper limit
##For each pi in the grid, find P(Y<=28)
probU<-pbinom(28,50,pis)

##Identify which of the pis yielded a prob less than or equal to 0.025
indexUall<-which(probU<=0.025)

##Which of those is closest to 0.025?
indexU<-min(indexUall)

##Which pi is that?
piU<-pis[indexU]

#Lower limit
##For each pi in the grid, find P(Y>=28)
probL<-pbinom(28,50,pis,lower.tail=F)

##Identify which of the pis yielded a prob less than or equal to 0.025
indexLall<-which(probL<=0.025)

##Which of those is closest to 0.025?
indexL<-max(indexLall)

##Which pi is that?
piL<-pis[indexL]

#Your interval
c(piL,piU)

# The above, but combined:
pis[min(which(pbinom(28,50,pis)<=0.025))]
pis[max(which(pbinom(28,50,pis,lower.tail=F)<=0.025))]

```

## One-sided Intervals

Thus far, we have discussed only "two-sided" intervals. These intervals have an upper and lower bound. Typically, $\alpha$ is apportioned equally between the two tails. (Thus, we look for $Z_{\alpha/2}$.) 

In "one-sided" intervals, we only bound the interval on one side. We construct one-sided intervals when we are concerned with whether a parameter exceeds some threshold, or whether a proportion exceeds 0.5, etc. Building a one-sided interval is similar to building two-sided intervals, except rather than dividing $\alpha$ into two, you simply apportion all of $\alpha$ to the relevant side.










