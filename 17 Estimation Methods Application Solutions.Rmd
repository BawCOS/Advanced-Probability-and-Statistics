---
title: "Estimation Methods Application Solutions"
author:
- Lt Col Ken Horton
- Professor Bradley Warner
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
   - \usepackage{multirow}
   - \usepackage{multicol}
output: 
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align='center')
knitr::opts_chunk$set(out.width = "75%")
library(knitr)
library(mosaic)
library(tidyverse)
```


\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\Prob}{\mbox{P}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

# Exercises

\indent 1. In the Notes, we found that if we take a sample from the uniform distribution $\textsf{Unif}(0,\theta)$, the method of moments estimate of $\theta$ is $\hat{\theta}_{MoM}=2\bar{x}$. Suppose our sample consists of the following values: 
$$
0.2 \hspace{0.4cm} 0.9 \hspace{0.4cm} 1.9 \hspace{0.4cm} 2.2 \hspace{0.4cm} 4.7 \hspace{0.4cm} 5.1
$$

a) What is $\hat{\theta}_{MoM}$ for this sample? 

```{r lesson20a}
x<-c(0.2,0.9,1.9,2.2,4.7,5.1)
thetamom<-2*mean(x)
thetamom
```

b) What is wrong with this estimate? 

The estimate for $\theta$ is 5, which is an impossible value for $\theta$, since one of our observations (5.1) is beyond this value. 

c) Show that this estimator is unbiased.

We need to show that 
$$
E\left( \hat{\theta}_{MoM} \right) = \theta
$$
We proceed as follows
$$
E \left(2\bar{X} \right) = 2 E\left( \sum{\frac{X_i}{n}} \right) = \frac{2}{n} E\left( \sum{X_i} \right)
$$
$$
=\frac{2}{n}  \sum{E\left(X_i \right)} =\frac{2}{n}\sum{\frac{\theta}{2}}=\frac{n\theta}{n}=\theta
$$
Notice that in performing this derivation, we treated $X$ as a random variable and not as $x$ a data value.

d) ADVANCED: Use simulation in `R` to find out how often this happens ($\hat{\theta}_{MoM} < \max x$). Report an answer for various sizes of samples.

Let's start by writing code for one sample size and then generalize.

```{r}
check <- function(n=10){
  temp<-runif(n,max=5)
  2*mean(temp)<max(temp)
}
```

```{r}
set.seed(3030)
check(10)
```

Now let's repeat

```{r}
(do(10000)*check(10)) %>%
  summarize(mean(check)) %>%
  pull()
```

Let's make `check` a vectorized function

```{r}
check <- Vectorize(check)
```

```{r}
my_data<-(do(1000)*check(seq(10,200,5))) %>%
  summarise_all(mean) %>%
  pivot_longer(everything(),names_to = "Sample",values_to = "Percent") %>%
  mutate(Sample=seq(10,200,5))
```

```{r}
my_data %>%
  gf_line(Percent~Sample,xlab="Sample Size",title="Percent When Estimator is Valid") %>%
  gf_theme(theme = theme_minimal())
```

Here is more traditional, old school `R` code.

```{r lesson20b}
simn<-function(n){
  y<-replicate(1000,{
    x<-runif(n)
    (2*mean(x))<max(x)
  })
  mean(y)
}

t<-seq(10,200,4)
persim<-sapply(t,simn)
plot(t,persim,type="l")
```


&nbsp;

\indent 2. Let $x_1,x_2,...,x_n$ be a simple random sample from an exponentially distributed population with parameter $\lambda$. Find $\hat{\lambda}_{MoM}$. 

Recall that $\E(X)={1\over \lambda}$. Thus,
$$
\hat{\lambda}_{MoM}={1\over \bar{x}}
$$


&nbsp;

\indent 3. Let $x_1,x_2,...,x_n$ be an iid random sample from an exponentially distributed population with parameter $\lambda$. Find $\hat{\lambda}_{MLE}$.

Recall that 
$$
f_X(x;\lambda)=\lambda e^{-\lambda x}
$$

So the likelihood function is: 
$$
L(\lambda;\boldsymbol{x})=\prod_{i=1}^n \lambda e^{-\lambda x_i}=\lambda^n e^{-\lambda\sum x_i}
$$

And the log-likelihood function is:
$$
l(\lambda;\boldsymbol{x})=n\log \lambda - \lambda \sum x_i
$$

Taking the derivative with respect to $\lambda$ and setting to 0:
$$
{\diff l(\lambda;\boldsymbol{x})\over \diff \lambda}={n\over \lambda}-\sum x_i =0
$$

Note that ${\diff^2 l(\lambda;\boldsymbol{x})\over\diff \lambda^2}=-{n\over \lambda^2}<0$, so $l$ is always concave down. Thus, any optimum found is a maximum. 

So, 
$$
\hat{\lambda}_{MLE}={n\over \sum x_i}={1\over \bar{x}}
$$



\indent 4. It is mathematically difficult to determine if the estimators found in questions 2 and 3 are unbiased. Since the sum is in the denominator, mathematically we may have to work with the joint pdf. So instead, use simulation to get an sense of whether the method of moments estimator for the exponential distribution is unbaised.

We need to sample data from an exponential and then compare the the reciprocal of the mean to the parameter.

```{r}
set.seed(630)
1/mean(rexp(1000,rate=10))
```

This is close, maybe we just got lucky. Let's repeat many times.

```{r}
(do(1000)*(1/mean(rexp(1000,rate=10)))) %>%
  gf_boxplot(~result)
```

Looks like it has the potential to be unbiased. We would need to investigate other values for $\lambda$.

\indent 5. Find a maximum likelihood estimator for $\theta$ when $X\sim\textsf{Unif}(0,\theta)$. Compare this to the method of moments estimator we found. Hint: Do not take the derivative of the likelihood function. 

$$
L(\theta;\boldsymbol{x})={1\over \theta^n}, \hspace{0.5cm} \mbox{only if all }x_i\leq \theta
$$

Recall that $f(x_i;\theta)={1\over \theta}$ if $x_i\in [0,\theta]$ and 0 elsewhere. And since the likelihood function is simply the product of these pdfs, if any $x_i$ is beyond $\theta$, then the entire likelihood function is 0. 

You can picture $L$ as a decreasing function of $\theta$, but remembering that $L$ takes the value 0 if $\theta$ is smaller than at least one $x_i$. Thus, $L$ achieves its maximum at $\theta=\max x_i$. 

This estimate is more intuitive than the method of moments estimate ($2\bar{x}$). The method of moments estimate is sometimes not feasible. Meanwhile, the MLE ($\max x_i$) is always feasible. 







### File Creation Information 

  * File creation date: `r Sys.Date()`
  * Windows version: `r win.version()`
  * `r R.version.string`
  * `mosaic` package version: `r packageVersion("mosaic")`
  * `tidyverse` package version: `r packageVersion("tidyverse")`
   * `fastR2` package version: `r packageVersion("fastR2")`