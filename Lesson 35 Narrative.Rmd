---
title: "Lesson 35: Simple Linear Regression IV: ANOVA"
author: "Lt Col Ken Horton"
date: "November 18, 2019"
header-includes: 
  - \usepackage{amsmath,multirow}
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align='center')
knitr::opts_chunk$set(out.width = "75%")
```

\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\Prob}{\mbox{P}}
\newcommand{\diff}{\,\mathrm{d}}


# Objectives

1) Build and interpret a simple linear model with one categorical predictor variable. 

2) Define, obtain and interpet an ANOVA table. 

3) Evaluate assumptions behind a linear model with one categorical predictor variable. 

# Categorical Predictors

Thus far, we have only discussed regression in the context of a quantitative, continuous response AND a quantitative, continuous predictor. We can build linear models with categorical predictor variables as well. 

In the case of a binary covariate, nothing about the linear model changes. The two levels of the binary covariate are typically coded as 1 and 0, and the model is built, evaluated and interpreted in an analogous fashion as before. 

In the case of a categorical covariate with $k$ levels, where $k>2$, we need to include $k-1$ *dummy variables* in the model. Each of these dummy variables takes the value 0 or 1. For example, if a covariate has $k=3$ categories or levels (say A, B or C), we create two dummy variables, $X_1$ and $X_2$, each of which can only take values 1 or 0. If $X_1=1$, the covariate takes the value A. If $x_2=1$, the covariate takes the value B. If both $X_1=0$ and $X_2=0$, this is known as the reference category, and in this case the covariate takes the value C. The arrangement of the levels of the categorical covariate are arbitrary and can be adjusted by the user.

The linear model is $Y=\beta_0 + \beta_1X_1 + \beta_2X_2+e$. 

When the covariate takes the value A, $\E(Y)=\beta_0 + \beta_1$. 

When the covariate takes the value B, $\E(Y)=\beta_0 + \beta_2$. 

When the covariate takes the value C, $\E(Y)=\beta_0$. 

Based on this, think about how you would interpret the coefficients $\beta_0$, $\beta_1$, and $\beta_2$. 

## Example 35.1

Suppose a pizza delivery company has three branches (East, West and Center). The data set `pizza_delivery.csv` contains a number of variables, but we are going to explore whether pizza delivery times differed depending on the branch at which the delivery originated. 

```{r lesson35a}
pizza<-read.csv("pizza_delivery.csv")
plot(pizza$branch,pizza$time)

#using ggplot:
#pizza%>%ggplot(aes(x=branch,y=time))+
#  geom_boxplot()

```

It appears that there is some evidence that delivery time depends on location. We can build a linear model to explore whether this difference in significant. We can use the `lm()` function in `R`, but in order to include a categorical predictor, we need to make sure that variable is stored as a "factor" type. If it is not, we'll need to convert it. 
```{r lesson35b}
class(pizza$branch)

#If already not of type factor, we can convert:
pizza$branch<-as.factor(pizza$branch)
```

Now we can build the model: 

```{r lesson35c}
pizza.lm<-lm(time~branch,data=pizza)
summary(pizza.lm)
```

Note that by default, `R` set the `Centre` level as the reference category. This is because it is first alphabetically. You can control this by changing the order of the factor levels. A quick internet search can help you do that if you need to. 

How would we interpret this output? Since `Centre` is reference category, the intercept is effectively the estimated delivery time for the Centre location. 

```{r lesson35d}
mean(pizza[pizza$branch=="Centre",]$time)
```

The other terms represent the expected difference in delivery times for the other locations. Specifically, on average, deliveries from the East location take 5.246 minutes less than deliveries from the Centre location. 

The `Std.Error`, `t value`, and `Pr(>|t|)` values can be used to conduct inference about the respective estimates. It appears that a significant difference in mean delivery time exists between the locations. 

## ANOVA Table

When building a linear model with a categorical predictor variable, it is common practice to build an *analysis of variance*, or ANOVA, table. It can be obtained in `R` by using the `anova()` function:

```{r lesson35e}
anova(pizza.lm)
```

This table lays out how the variation between observations is broken down. Similar to how $R$-squared compares variation explained by the model with overall variation in response, the ANOVA table compares variation between categories to variation within categories. The ratio of `Mean Sq` for the predictor variable to the `Mean Sq` for residuals is equivalent to the `F value`. For large values of this $F$-statistic, we would reject the null hypothesis of no differences in mean response across levels of the categorical variable. 

## Pairwise Comparisons

The ANOVA table above (along with the summary of the linear model output before that) merely tells you whether any difference exists in the mean response across the levels of the categorical predictor. It does not tell you where that difference lies. In the case of our example, we could not use the summaries above to determine whether the East location was significantly faster than the West location, etc. In order to make this comparison, we need to use some method of performing pairwise comparisons. A common one is the Tukey method. Essentially, the Tukey method conducts three hypothesis tests (each under the null of no difference in mean) but corrects the $p$-values based on the understanding that we are conducting three simultaneous hypothesis tests with the same set of data. 

We can obtain these pairwise comparisons using the `TukeyHSD()` function in `R`. The "HSD" stands for "Honest Significant Differences". This function requires an `anova` object, which is obtained by using the `aov()` function:
```{r lesson35f}
TukeyHSD(aov(time~branch, data=pizza))
```

According to this output, all three branches have significantly different mean delivery times than the others. Note that this result is partly due to the large sample size ($n=1266$). One would have to ask whether a difference in mean delivery is 1.118 minutes is practically relevant in this context. 

## Assumptions

Keep in mind that ANOVA is a special case of a simple linear model. Therefore, all of the assumptions remain the same. In order to evaluate these assumptions, we would need to obtain the appropriate diagnostic plots: 
```{r lesson35g}
plot(pizza.lm)
```



