---
title: "Lesson 24 Applications"
author: "Lt Col Ken Horton"
date: "October 15, 2019"
header-includes: 
  - \usepackage{amsmath,multirow}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align='center')
knitr::opts_chunk$set(out.width = "75%")
```


\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\Prob}{\mbox{P}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

# Exercises

\indent 1. Consider Example 24.1 from the narrative. Using a $t$-based interval, we found a 95% confidence interval of (67.194,70.206). 

a) First, think about your answers to practical application problem 23.2c and 23.2d. We found that the probability that we obtained our sample (or more extreme) given a population mean of 67 inches was 0.0278. Furthermore, the value 67 is not included in the interval found in Example 24.1. Do these two results make sense? Or do they conflict with each other? Explain. 

These two results are consistent. Both are suggesting that 67 inches is not a feasible value of $\mu$. 

b) Based on the same sample, build 80%, 90% and 99% confidence intervals. What do you notice about the intervals as confidence level changes? 
```{r lesson24c}
xbar<-68.7
s<-5.3
n<-50

#80% Ci
a<-0.2
tval<-qt(1-a/2,49)
xbar+c(-1,1)*tval*s/sqrt(n)

#90% Ci
a<-0.1
tval<-qt(1-a/2,49)
xbar+c(-1,1)*tval*s/sqrt(n)

#99% Ci
a<-0.01
tval<-qt(1-a/2,49)
xbar+c(-1,1)*tval*s/sqrt(n)

```

As the confidence level goes up, the intervals get wider. 

c) Based on the same sample, build 95% confidence intervals assuming the sample was based on $n=10$, $n=30$, and $n=100$. Assume the same sample mean and standard deviation. 

```{r lesson24d}
xbar<-68.7
s<-5.3
a<-0.05

#n=10
n<-10
tval<-qt(1-a/2,49)
xbar+c(-1,1)*tval*s/sqrt(n)

#n=30
n<-30
tval<-qt(1-a/2,49)
xbar+c(-1,1)*tval*s/sqrt(n)

#n=100
n<-100
tval<-qt(1-a/2,49)
xbar+c(-1,1)*tval*s/sqrt(n)
```

As $n$ increases, the interval widths get smaller. This makes sense; as we get more information, we get a better picture of the underlying population. 


&nbsp;

\indent 2. Let's stay with the same example. Suppose the true mean height at our college is $\mu=67$ inches and the true standard devation is $\sigma =4.9$. 

a) In theory, in 95% of samples collected from the population, the 95% confidence interval will contain the true mean. Obtain a random sample of size 50 from the population, then use that sample to build a 95% confidence interval for $\mu$. Assume you don't know the true standard deviation (so you'll use the interval based on the $t$-distribution, just like in Example 24.1). Did that interval contain the true value of $\mu$ (67 inches)? Repeat that process 10000 times and report the percentage of time the 95% confidence interval contained the true mean.  

```{r lesson24a}
set.seed(2003)
sim_cover<-replicate(10000,{
  sam<-rnorm(50,67,4.9)
  ci<-mean(sam)+c(-1,1)*qt(0.975,49)*sd(sam)/sqrt(50)
  covered<-(ci[1]<=67)*(ci[2]>=67)
  covered
})

mean(sim_cover)

```

The 95% confidence interval contained the true mean 95.28% of the time. 

b) Repeat part a), but instead of using $t$-based intervals, use normal-based intervals. In this case, we will be using our sample standard deviation and just assuming that will be equal to $\sigma$. How does this change the coverage of the confidence intervals? Note that to make a fair comparison, you will need to use the same simulated data. Make use of the `set.seed()` function to do this. 

```{r lesson24b}
set.seed(2003)
sim_cover<-replicate(10000,{
  sam<-rnorm(50,67,4.9)
  ci<-mean(sam)+c(-1,1)*qt(0.975,49)*sd(sam)/sqrt(50)
  ci2<-mean(sam)+c(-1,1)*qnorm(0.975)*sd(sam)/sqrt(50)
  covered<-(ci[1]<=67)*(ci[2]>=67)
  covered2<-(ci2[1]<=67)*(ci2[2]>=67)
  c(covered,covered2)
})

apply(sim_cover,1,mean)

set.seed(2003)
sim_cover<-replicate(10000,{
  sam<-rnorm(50,67,4.9)
  ci<-mean(sam)+c(-1,1)*qt(0.975,49)*sd(sam)/sqrt(50)
  covered<-(ci[1]<=67)*(ci[2]>=67)
  covered
})

set.seed(2003)
sim_cover2<-replicate(10000,{
  sam<-rnorm(50,67,4.9)
  ci2<-mean(sam)+c(-1,1)*qnorm(0.975)*sd(sam)/sqrt(50)
  covered2<-(ci2[1]<=67)*(ci2[2]>=67)
  covered2
})

mean(sim_cover)
mean(sim_cover2)

```

The coverage percentage dropped to 94.78%. It turns out that, overall, if you make the inappropriate assumption that $\sigma=s$, your 95% confidence interval is not actually a 95% interval. 

&nbsp;


\indent 3. In Example 24.2, we directly used the actual distribution of $Y$ (the number of respondents who support a measure) to find a confidence interval for $\pi$. Alternatively, we could use the central limit theorem to find an approximate distribution of $\hat{\pi}$, and use that to build a confidence interval. 

a) Why would we need to use this approximation to find a confidence interval for $\pi$? Hint: imagine you didn't have access to `pbinom` or anything comparable (or even a computer). 

Standard normal tables were easier to use than the binomial distribution for large $n$. It would be very time consuming to find all the relevant probabilities to answer this question. 

b) Use the normal approximation to find a 95% confidence interval for $\pi$. Use two methods: the "Wald" interval and the "Score" or "Wilson" interval. You can consult IPSUR or the resource of your choice for more information. 

By the central limit theorem, we know that
$$
{\hat{\pi}-\pi\over\sqrt{\pi(1-\pi)\over n}}\sim \textsf{Norm}(0,1)
$$

The Wald interval uses this expression directly:
$$
\pi \in \left(\hat{\pi}\pm z_{\alpha/2}\sqrt{\hat{\pi}(1-\hat{\pi})\over n}\right)
$$

```{r lesson24e}
pihat<-28/50
n<-50

pihat+c(-1,1)*qnorm(0.975)*sqrt(pihat*(1-pihat)/n)
```

The score or Wilson interval acknowledges that we are using an estimate of $\pi$ to estimate standard deviation and adjusts accordingly:
```{r lesson24f}
pihat<-28/50
n<-50
z<-qnorm(0.975)
(pihat+z^2/(2*n)+c(-1,1)*z*sqrt(pihat*(1-pihat)/n + z^2/(4*n^2)))/(1+z^2/n)
```

c) BONUS: Conduct a simulation to compare the three methods of finding a 95% confidence interval. Let $n=50$ and consider different values of $\pi$. 
```{r lesson24g}
pis<-seq(0.00,1,0.001)
pitrue<-0.5
n<-50
z<-qnorm(0.975)

set.seed(2003)
sim50<-replicate(10000,{
  x<-rbinom(1,n,pitrue)
  pihat<-x/n
  
  bint<-c(pis[max(which(pbinom(x,n,pis,lower.tail=F)<=0.025))],
          pis[min(which(pbinom(x,n,pis)<=0.025))])
  wint<-pihat+c(-1,1)*z*sqrt(pihat*(1-pihat)/n)
  sint<-(pihat+z^2/(2*n)+c(-1,1)*z*sqrt(pihat*(1-pihat)/n +
                                          z^2/(4*n^2)))/(1+z^2/n)
  
  c((bint[1]<=pitrue)*(bint[2]>=pitrue),
    (wint[1]<=pitrue)*(wint[2]>=pitrue),
    (sint[1]<=pitrue)*(sint[2]>=pitrue))

})

apply(sim50,1,mean)



pitrue<-0.25
set.seed(2003)
sim25<-replicate(10000,{
  x<-rbinom(1,n,pitrue)
  pihat<-x/n
  
  bint<-c(pis[max(which(pbinom(x,n,pis,lower.tail=F)<=0.025))],
          pis[min(which(pbinom(x,n,pis)<=0.025))])
  wint<-pihat+c(-1,1)*z*sqrt(pihat*(1-pihat)/n)
  sint<-(pihat+z^2/(2*n)+c(-1,1)*z*sqrt(pihat*(1-pihat)/n +
                                          z^2/(4*n^2)))/(1+z^2/n)
  
  c((bint[1]<=pitrue)*(bint[2]>=pitrue),
    (wint[1]<=pitrue)*(wint[2]>=pitrue),
    (sint[1]<=pitrue)*(sint[2]>=pitrue))

})

apply(sim25,1,mean)


pitrue<-0.1
set.seed(2003)
sim10<-replicate(10000,{
  x<-rbinom(1,n,pitrue)
  pihat<-x/n
  
  bint<-c(pis[max(which(pbinom(x,n,pis,lower.tail=F)<=0.025))],
          pis[min(which(pbinom(x,n,pis)<=0.025))])
  wint<-pihat+c(-1,1)*z*sqrt(pihat*(1-pihat)/n)
  sint<-(pihat+z^2/(2*n)+c(-1,1)*z*sqrt(pihat*(1-pihat)/n +
                                          z^2/(4*n^2)))/(1+z^2/n)
  
  c((bint[1]<=pitrue)*(bint[2]>=pitrue),
    (wint[1]<=pitrue)*(wint[2]>=pitrue),
    (sint[1]<=pitrue)*(sint[2]>=pitrue))

})

apply(sim10,1,mean)


```

