---
title: "Lesson 36: Multiple Linear Regression"
author: "Lt Col Ken Horton"
date: "November 20, 2019"
header-includes: 
  - \usepackage{amsmath,multirow}
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align='center')
knitr::opts_chunk$set(out.width = "75%")
```

\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\Prob}{\mbox{P}}
\newcommand{\diff}{\,\mathrm{d}}


# Objectives

1) 

2) 

3) 

# Regression with Multiple Predictors

Over the last four lessons, we have introduced and explored basic linear models. We dove in to estimation techniques (least squares) and model inference based on the normality assumptions of the model. However, throughout those lessons, we have only included one predictor variable in the model at a time. We may want to predict a response based on multiple predictor variables rather than just one. For example, the stopping distance of a car may depend on other factors besides speed. The simple linear regression model can be expanded to include multiple predictors. The resulting model is referred to as *multiple linear regression*.

## Reference

As a reference, I used section 11.6 of "Introduction to Statistics and Data Analysis" by Heumann, Schomaker and Shalabh. I encourage you to seek out this or another text, or research the topic online if you would like to seek further explanation of clarification on multiple linear regression. 

## The Model

The simple linear regression model can be extended to include more than one predictor: 
$$
Y= \beta_0 + \beta_1X_1+\beta_2X_2+...+\beta_pX_p+e
$$

If we have a set of $n$ observations that follow this model, we can rewrite this as a sequence of expressions:
$$
\begin{array}{c}
y_1=\beta_0 + \beta_1x_{11} + \beta_2x_{12} + ... + \beta_px_{1p} + e_1 \\
y_2=\beta_0 + \beta_1x_{21} + \beta_2x_{22} + ... + \beta_px_{2p} + e_2 \\
\vdots \\
y_n=\beta_0 + \beta_1x_{n1} + \beta_2x_{n2} + ... + \beta_px_{np} + e_n
\end{array}
$$

It is helpful to write this in matrix notation:
$$
\textbf{y}=\textbf{X}\boldsymbol{\beta}+\textbf{e}
$$

where
$$
\textbf{y}=\left(\begin{array}{c} y_1\\ y_2\\ \vdots \\ y_n \end{array}\right), \hspace{0.5cm} \textbf{X}=\left(\begin{array}{ccccc} 1 & x_{11} & x_{12} & ... & x_{1p}\\ 1 & x_{21} & x_{22} & ... & x_{2p}\\ \vdots \\ 1 & x_{n1} & x_{n2} & ... & x_{np} \end{array}\right), \hspace{0.5cm} \boldsymbol{\beta}=\left(\begin{array}{c} \beta_0\\ \beta_1\\ \vdots \\ \beta_p \end{array}\right), \hspace{0.5cm} \textbf{e} = \left(\begin{array}{c} e_1\\ e_2\\ \vdots \\ e_n \end{array}\right)
$$

The matrix $\textbf{X}$ is referred to as the *design matrix*. The least squares estimate of $\beta$ is given by:
$$
\hat{\boldsymbol{\beta}} = (\textbf{X}'\textbf{X})^{-1}\textbf{X}'\textbf{y}
$$

## Assumptions and Inference

The assumptions behind multiple linear regression are similar to those behind simple linear regression. We assume that the error term is normally distributed:
$$
\textbf{e}\sim \textsf{Norm}(\boldsymbol{0},\sigma\textbf{I})
$$

where $\textbf{I}$ is the identity matrix and $\sigma$ is constant across values of $\textbf{X}$. Note that the normal distribution in this case is the multivariate version of the distribution. In other words, the errors are jointly normally distributed. 

Because of this assumption of normality, we can determine the distribution of our coefficient estimates:
$$
\hat{\boldsymbol \beta} \sim \textsf{Norm}\left(\boldsymbol \beta,\sigma\sqrt{(\textbf{X}'\textbf{X})^{-1}}\right)
$$

We can use this to conduct inference about coefficients. First, we need to obtain an estimate of $\sigma$:
$$
\hat{\sigma}=\sqrt{{1\over n-(p+1)}\sum_{i=1}^n \hat{e}_i^2}
$$

where $\hat{e}_i$ is the observed $i$th residual. 

From this, we can derive a 95% confidence interval for each $\beta_j$:
$$
\hat{\beta}_j \pm t_{n-p-1;0.975}\hat\sigma_{\hat{\beta}_j}
$$

where $\hat\sigma_{\hat{\beta}_j} = \sqrt{s_{jj}\hat{\sigma}^2}$ where $s_{jj}$ is the $j$th diagonal element of the matrix $(\textbf{X}'\textbf{X})^{-1}$. Furthermore, we can use the test statistic $\hat{\beta}_j/\hat{\sigma}_{\hat{\beta}_j}$ and compare it to the $\textsf{t}(n-p-1)$ distribution to test the hypothesis $H_0:\beta_j=0$. 

Another often overlooked assumption in multiple linear regression is that the predictor variables are mutually uncorrelated. If predictor variables are correlated, *multicollinearity* is said to exist. This leads to large variance in parameter estimates, making useful inference difficult. 

## Application in R

After four lessons of linear models, you are probably suspecting that all of the above can be accomplished directly in `R`. You would be correct. 

### Example 36.1

The `Carseats` data contains information on child car seat sales at 400 different stores. It is part of the `ISLR` package. Build a linear regression model fitting `Sales` against advertising budget (`Advertising`), carseat price (`Price`), and shelving location quality (`ShelveLoc`). 
```{r lesson36a}
my.model<-lm(Sales~Advertising+Price+ShelveLoc,data=Carseats)
summary(my.model)

```

Note that the variable `ShelveLoc` is categorical, so the model is given by:
$$
\E(\text{Sales})=11.47 + 0.11*\text{Advertising} -0.06*\text{Price}+4.78*\text{(ShelveLoc=Good)}+ 1.83*\text{(ShelveLoc=Medium)}
$$

According to the summary, each of the predictor variables is significant in explaining variation in carseat sales. 

The coefficient estimates are interpreted in the same way as in simple linear regression; however, when interpreting one coefficient estimate, it is assumed that all other variables remain constant. For example, for an increase in advertising of 1 (or \$1000), we expect to see an average increase in sales of 0.11 (or \$110), assuming all other variables remain constant. 

Keep in mind that we can apply the `plot()` function to the `lm` object to obtain diagnostic plots for the linear model. These plots have the same interpretation as in simple regression. 

## Prediction

In multiple regression, prediction is performed in a similar fashion. Confidence intervals and prediction intervals around an estimated response given specific values of the predictor variables can be found using the `predict()` function. Suppose we wanted to know the expected sales for a store with an advertising budget of \$10,000, a price value of \$90, and a good quality shelving location: 

```{r lesson 36b}
predict(my.model,newdata = data.frame(Advertising=10,Price=90,ShelveLoc="Good"),
        interval = "confidence")
predict(my.model,newdata = data.frame(Advertising=10,Price=90,ShelveLoc="Good"),
        interval = "prediction")
```

## Interaction and Higher Order Terms

Suppose we suspected that the relationship between sales and price differed based on quality of the shelving location. In this case, we would want to include an interaction term in the model: 
$$
\E(Sales)=\beta_0 + \beta_1*\text{Advertising}+ \beta_2*\text{Price}+ \beta_3*\text{(ShelveLoc=Good)}+ 
$$
$$
\beta_4*\text{(ShelveLoc=Medium)} + \beta_5*\text{Price}*\text{(ShelveLoc=Good)} + \beta_6*\text{Price}*\text{(ShelveLoc=Medium)} 
$$

To include an interaction term when building a model in `R`, we simply need to include the product of the two variables as a predictor. IMPORTANT: If we include interaction terms, we must also include the individual predictor variables as well. 
```{r lesson36c}
my.model2<-lm(Sales~Advertising+Price+ShelveLoc+Advertising*ShelveLoc,data=Carseats)
summary(my.model2)
```

Furthermore, if you suspect that perhaps a curved relationship exists between two variables, we could include a higher order term. As an example, let's add a quadratic term for `Price` to our model (without the interaction). To do this in `R`, we need to wrap the higher order term in `I()`. Again, if we include a higher order term, we need to include the lower order terms as well:
```{r lesson36d}
my.model3<-lm(Sales~Advertising+Price+I(Price^2)+ShelveLoc,data=Carseats)
summary(my.model3)
```

## Model Comparison

When working through the last two examples, you may have noticed that the added terms were not statistically significant. Furthermore, you could also notice that the adjusted $R$-squared values did not change substantially. Recall that the $R$-squared value measures fit of a model. The larger this value, the better the data fit to the model. When adding the interaction and higher order terms to our example, we did not see a substantial improvement. Therefore, it wouldn't be worth it to include these terms in the model.

I won't go into great detail regarding this topic, since we will dive into linear regression model comparison early in Math 378.


