---
title: "Named Continuous Distribution Notes"
author:
- Lt Col Ken Horton
- Professor Bradley Warner
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos='ht')
library(mosaic)
library(tidyverse)
```

\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\Prob}{\mbox{P}}
\newcommand{\diff}{\,\mathrm{d}}


## Objectives

1) Recognize common continuous distributions (Uniform, Exponential, Gamma, Normal, Beta, Weibull).  

2) Use `R` to calculate probabilities and quantiles involving random variables with common continuous distributions.

3) Describe the relationship between the Poisson process and the Poisson & Exponential distributions. 

4) Describe the memoryless property. 

## Continuous Distributions 

In this lesson we will explore continuous distributions. This means we work with probability density functions and to find probabilities we must integrate, either numerical, graphically, or mathematically. The cumulative distribution function will also play an important role. There are many more distributions than the ones in this lesson but these are the most common and will set you up to learn and use any others in the future.

### Uniform Distribution

The first continuous distribution we will discuss is the uniform distribution. By default, when we refer to the uniform distribution, we are referring to the continuous version. When referring to the discrete version, we use the full term "discrete uniform distribution."

A continuous random variable has the uniform distribution if probability is evenly distributed throughout the sample space. The parameters of this distribution are $a$ and $b$, representing the minimum and maximum of the sample space. It is commonly denoted as $U(a,b)$.

Let $X$ be a continuous random variable with the uniform distribution. This is denoted as $X\sim \textsf{Unif}(a,b)$. The pdf of $X$ is given by:
$$
f_X(x)=\left\{\begin{array}{ll} \frac{1}{b-a}, & a\leq x \leq b \\ 0, & \mbox{otherwise} \end{array}\right.
$$

The mean of $X$ is $\E(X)=\frac{a+b}{2}$ and the variance is $\Var(X)=\frac{(b-a)^2}{12}$. The derivations of these quantities is left to the practical application. 

The most common uniform distribution is $U(0,1)$ which we have already used several times in this course. Notice that the plot of the **pdf** is constant or uniform.

```{r}
gf_dist("unif")
```

To check it is a proper pdf, all values must be non-negative and the total probability must be 1.

```{r}
integrate(function(x)dunif(x),0,1)
```


### Exponential Distribution

Recall from the lesson on named discrete distributions, we discussed the Poisson process. If arrivals follow a Poisson process, we know that the number of arrivals in a specified amount of time follows a Poisson distribution, and the time until the next arrival follows the *exponential* distribution. So in the Poisson, the number of arrivals is random and the interval is fixed. In the exponential we change this, the interval is random and the arrivals are fixed at 1. This is a subtle point but worth the time to make sure you understand. 

Let $X$ be the number of arrivals in a time interval $T$, where arrivals occur according to a Poisson process with an average of $\lambda$ arrivals per time interval. From the previous lesson, we know that $X\sim \textsf{Poisson}(\lambda T)$. Now let $Y$ be the time until the next arrival. Then $Y$ follows the exponential distribution with parameter $\lambda$ which has unit of inverse base time unit:
$$
Y \sim \textsf{Expon}(\lambda)
$$

Note on $\lambda$: One point of confusion involving the parameters of the Poisson and exponential distributions. The parameter of the Poisson distribution (usually denoted as $\lambda$) represents the average number of arrivals in whatever amount of time specified by the random variable. In the case of the exponential distribution, the parameter (also denoted as $\lambda$) represents the average number of arrivals per unit time. For example, suppose arrivals follow a Poisson process with an average of 10 arrivals per day. $X$, the number of arrivals in 5 days, follows a Poisson distribution with parameter $\lambda=50$, since that is the average number of arrivals in the amount of time specified by $X$. Meanwhile, $Y$, the time in days until the next arrival, follows an exponential distribution with parameter $\lambda=10$ (the average number of arrivals per day). 

The pdf of $Y$ is given by:
$$
f_Y(y)=\lambda e^{-\lambda y}, \hspace{0.3cm} y>0
$$

The mean and variance of $Y$ are: $\E(Y)=\frac{1}{\lambda}$ and $\Var(Y)=\frac{1}{\lambda^2}$. The derivations are left as a practical application exercise. 

> *Example*:  
Suppose at a local retail store, customers arrive to a checkout counter according to a Poisson process with an average of one arrival every three minutes. Let $Y$ be the time (in minutes) until the next customer arrives to the counter. What is the distribution (and parameter) of $Y$? What are $\E(Y)$ and $\Var(Y)$? Find $\Prob(Y>5)$, $\Prob(Y\leq 3)$, and $\Prob(1 \leq Y < 5)$? Also, find the median and 95th percentile of $Y$. Finally, plot the pdf of $Y$. 

Since one arrival shows every three minutes, the average number of arrivals per unit time is 1/3 arrival per minute. Thus, $Y\sim \textsf{Expon}(\lambda=1/3)$. This means that $\E(Y)=3$ and $\Var(Y)=9$. 

To find $\Prob(Y>5)$, we could integrate the pdf of $Y$:
$$
\Prob(Y>5)=\int_5^\infty \frac{1}{3}e^{-\frac{1}{3}y}\diff y = -e^{-\frac{1}{3}y}\bigg|_5^\infty=0-(-e^{-\frac{5}{3}})= 0.189
$$

Alternatively, we could use `R`:
```{r lesson11a}
##Prob(Y>5)=1-Prob(Y<=5)
1-pexp(5,1/3)
```

Or using `integrate()`

```{r}
integrate(function(x)1/3*exp(-1/3*x),5,Inf)
```


For the remaining probabilities, we will use `R`:
```{r lesson11b}
##Prob(Y<=3)
pexp(3,1/3)

##Prob(1<=Y<5)
pexp(5,1/3)-pexp(1,1/3)

```

The median is $y$ such that $\Prob(Y\leq y)=0.5$. We can find this by solving the following for $y$:
$$
\int_0^y \frac{1}{3}e^{-\frac{1}{3}y}\diff y = 0.5
$$

Alternatively, we can use `qexp` in `R`:
```{r lesson11c}
##median
qexp(0.5,1/3)

##95th percentile
qexp(0.95,1/3)
```

```{r ,fig.align='center',echo=F,fig.cap="pdf of $Y$"}
gf_dist("exp")
```

### Memoryless Property

The Poisson process is known for its *memoryless* property. Essentially, this means that the time until the next arrival is independent of time since last arrival. Thus, the probability of an arrival within the next 5 minutes is the same regardless of whether an arrival just occurred or an arrival has not occurred for a long time. 

To show this, let's consider random variable $Y$ (in minutes) where $Y\sim\textsf{Expon}(\lambda)$. I will show that, given it has been at least $t$ minutes since the last arrival, the probability we wait at least $y$ additional minutes is equal to the marginal probability that we wait $y$ additional minutes. 

First, note that the cdf of $Y$, $F_Y(y)=\Prob(Y\leq y)=1-e^{-\lambda y}$. So,
$$
\Prob(Y\geq y+t|Y\geq t) = \frac{\Prob(Y\geq y+t \cap Y\geq t)}{\Prob(Y\geq t)}=\frac{\Prob(Y\geq y +t)}{\Prob(Y\geq t)} = \frac{1-(1-e^{-(y+t)\lambda})}{1-(1-e^{-t\lambda})}
$$
$$
=\frac{e^{-\lambda y }e^{-\lambda t}}{e^{-\lambda t }}=e^{-\lambda y} = 1-(1-e^{-\lambda y})=\Prob(Y\geq y)
$$

Let's simulate values for a Poisson. The Poisson is often used in modeling customer service situations such as service at Chipotle. However, some people has the mistaken idea that arrivals will be equally spaced. In fact, arrivals will come in clusters and bunches. Maybe this is the root of the common expression, "Bad news comes in threes"?  

```{r echo=FALSE}
set.seed(9)
runs<-8
size <- 40
time <- replicate(runs,cumsum(rexp(size)))
df <- data.frame(time=as.vector(time),run=rep(1:runs,each=size))
stop<-min(apply(time,2,max))
stop <- 5 * trunc(stop/5)
lattice::stripplot(run~time,df,pch=1,cex=.7,
col='black',
panel=function(x,y,...){
  panel.abline(h=seq(1.5,7.5,by=1),col='gray60')
  panel.abline(v=seq(0,stop,by=5),col='gray60')
  panel.stripplot(x,y,...)
})
```

The number of events in a box is $X\sim \textsf{Poisson}(\lambda = 5)$. As you can see, some boxes have more than 5 and some less because 5 is the average number of arrivals. Also note that the spacing is not equal. The runs are just repeated simulations of the same process.


### Gamma Distribution

The gamma distribution is a generalization of the exponential distribution. In the exponential distribution, the parameter $\lambda$ is sometimes referred to as the *rate* parameter. The gamma distribution is sometimes used to model wait times (as with the exponential distribution), but in cases without the memoryless property. The gamma distribution has two parameters, *rate* and *shape*. In some texts, *scale* (the inverse of rate) is used as an alternative parameter to rate. 

Suppose $X$ is a random variable with the gamma distribution with shape parameter $\alpha$ and rate parameter $\lambda$:
$$
X \sim \textsf{Gamma}(\alpha,\lambda)
$$

$X$ has the following pdf:
$$
f_X(x)=\frac{\lambda^\alpha}{\Gamma (\alpha)}x^{\alpha-1}e^{-\lambda x}, \hspace{0.3cm} x>0
$$

and 0 otherwise. The mean and variance of $X$ are $\E(X)=\frac{\alpha}{\lambda}$ and $\Var(X)=\frac{\alpha}{\lambda^2}$. Looking at the pdf, the mean and the variance, one can easily see that if $\alpha=1$, the resulting distribution is equivalent to $\textsf{Expon}(\lambda)$.  

#### Gamma Function

You may have little to no background with the Gamma function ($\Gamma (\alpha)$). This is different from the gamma distribution. The gamma function is simply a function and is defined by:
$$
\Gamma (\alpha)=\int_0^\infty t^{\alpha-1}e^{-t}\diff t
$$

There are some important properties of the gamma function. Notably, $\Gamma (\alpha)=(\alpha-1)\Gamma (\alpha -1)$, and if $\alpha$ is a non-negative integer, $\Gamma(\alpha)=(\alpha-1)!$. 

Suppose $X \sim \textsf{Gamma}(\alpha,\lambda)$. The pdf of $X$ for various values of $X$ is shown below. 
```{r lesson11e,echo=F,fig.height=4,fig.width=5,fig.align='center',fig.cap="pdf of Gamma for various values of alpha and lambda"}
curve(dgamma(x,1,1),from=0,to=10,ylab="f(x)")
curve(dgamma(x,1,5),from=0,to=10,add=T,col=2)
curve(dgamma(x,5,1),from=0,to=10,add=T,col=3)
curve(dgamma(x,10,2),from=0,to=10,add=T,col=4)
curve(dgamma(x,24,3),from=0,to=10,add=T,col=5)
legend("topright",c("a=1,lam=1","a=1,lam=5","a=5,lam=1","a=10,lam=2","a=24,lam=3"),lty=1,col=1:5,cex=0.75)
```


> *Example*:  
Let $X \sim \textsf{Gamma}(\alpha=5,\lambda=1)$. Find the mean and variance of $X$. Also, compute $\Prob(X\leq 2)$ and $\Prob(1\leq X < 8)$. Find the median and 95th percentile of $X$. 

The mean and variance of $X$ are $\E(X)=5$ and $\Var(X)=5$. To find probabilities and quantiles, integration will be difficult, so it's best to use the built-in `R` functions:
```{r lesson11f}
## Prob(X<=2)
pgamma(2,5,1)

##Prob(1 <= X < 8)
pgamma(8,5,1)-pgamma(1,5,1)

## median
qgamma(0.5,5,1)

## 95th percentile
qgamma(0.95,5,1)

```

### Normal Distribution

The normal distribution (also referred to as Gaussian) is a common distribution found in natural processes. You have likely seen a *bell curve* in various contexts. The bell curve is often indicative of an underlying normal distribution. There are two parameters of the normal distribution: $\mu$ (the mean of $X$) and $\sigma$ (the standard deviation of $X$). 

Suppose a random variable $X$ has a normal distribution with parameters $\mu$ and $\sigma$. The pdf of $X$ is given by:
$$
f_X(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, \hspace{0.3cm} -\infty < x <\infty
$$

```{r lesson11g,fig.height=4,fig.width=5,fig.align='center',echo=F,fig.cap="pdf of Normal for various values of mu and sigma"}
curve(dnorm(x,0,1),from=-7,to=7,ylim=c(0,0.8),ylab="f(x)")
curve(dnorm(x,2,1),from=-7,to=7,add=T,col=2)
curve(dnorm(x,0,3),from=-7,to=7,add=T,col=3)
curve(dnorm(x,0,0.5),from=-7,to=7,add=T,col=4)
curve(dnorm(x,-1,1),from=-7,to=7,add=T,col=5)
legend("topright",c("mu=0,sig=1","mu=2,sig=1","mu=0,sig=3","mu=0,sig=0.5","mu=-1,sig=1"),lty=1,col=1:5,cex=0.75)
```

#### Standard Normal

When random variable $X$ is normally distributed with $\mu=0$ and $\sigma=1$, $X$ is said to follow the *standard normal* distribution. Sometimes, the standard normal pdf is denoted by $\phi(x)$. 

Note that any normally distributed random variable can be transformed to have the standard normal distribution. Let $X \sim \textsf{Norm}(\mu,\sigma)$. Then,
$$
Z=\frac{X-\mu}{\sigma} \sim \textsf{Norm}(0,1)
$$

Partially, one can show this is true by noting that the mean of $Z$ is 0 and the variance (and standard deviation) of $Z$ is 1:
$$
\E(Z)=\E\left(\frac{X-\mu}{\sigma}\right)=\frac{1}{\sigma}\left(\E(X)-\mu\right)=\frac{1}\sigma(\mu-\mu)=0
$$
$$
\Var(Z)=\Var\left(\frac{X-\mu}{\sigma}\right)=\frac{1}{\sigma^2}\left(\Var(X)-0\right)=\frac{1}{\sigma^2} \sigma^2=1
$$

Note that this does not prove that $Z$ follows the standard normal distribution; we have merely shown that $Z$ has a mean of 0 and a variance of 1. We will discuss distributions of transformations in a later lesson. 

> *Example*:  
Let $X \sim \textsf{Norm}(\mu=200,\sigma=15)$. Compute $\Prob(X\leq 160)$, $\Prob(180\leq X < 230)$, and $\Prob(X>\mu+\sigma)$. Find the median and 95th percentile of $X$. 

As with the gamma distribution, to find probabilities and quantiles, integration will be difficult, so it's best to use the built-in `R` functions:
```{r lesson11h}
## Prob(X<=160)
pnorm(160,200,15)

##Prob(180 <= X < 230)
pnorm(230,200,15)-pnorm(180,200,15)

##Prob(X>mu+sig)
1-pnorm(215,200,15)

## median
qnorm(0.5,200,15)

## 95th percentile
qnorm(0.95,200,15)

```

### Beta distribution

Another common continuous distribution is the beta distribution. This has a unique application in that the domain of variable with the beta distribution is $[0,1]$. The beta distribution has two parameters, $\alpha$ and $\beta$. (In `R`, these are denoted as `shape1` and `shape2`.) 

Let $X \sim \textsf{Beta}(\alpha,\beta)$. The pdf of $X$ is given by: 
$$
f_X(x)=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}, \hspace{0.3cm} 0\leq x \leq 1
$$

In some resources, $\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}$ is written as $\frac{1}{B(\alpha,\beta)}$, where $B$ is known as the beta function. 

Note that $\E(X)=\frac{\alpha}{\alpha+\beta}$ and $\Var(X)=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$. 

For various values $\alpha$ and $\beta$, the pdf of a beta distributed random variable is shown below.

```{r lesson11i,fig.height=4,fig.width=5,fig.align='center',echo=F,fig.cap="pdf of Beta for various values of alpha and beta"}
curve(dbeta(x,0.5,0.5),from=0.01,to=.99,ylim=c(0,2.5),ylab="f(x)")
curve(dbeta(x,5,1),from=0.01,to=.99,add=T,col=2)
curve(dbeta(x,1,3),from=0.01,to=.99,add=T,col=3)
curve(dbeta(x,2,2),from=0.01,to=.99,add=T,col=4)
curve(dnorm(x,2,5),from=0.01,to=.99,add=T,col=5)
legend("top",c("a=0.5,b=0.5","a=5,b=1","a=1,b=3","a=2,b=2","a=2,b=5"),lty=1,col=1:5,cex=0.75)
```

### Weibull Distribution

The last common distribution we will explore is the Weibull distribution. Like the gamma, the Weibull distribution is a generalization of the exponential distribution and is meant to model wait times. A random variable with the Weibull distribution has parameters $\alpha$ and $\beta$. In `R`, these are referred to as `shape` and `scale` respectively. Note that in some resources, these are represented by $k$ and $\lambda$ or even $k$ and $\theta$. 
Let $X \sim \textsf{Weibull}(\alpha,\beta)$. The pdf of $X$ is given by:
$$
f_X(x)=\frac{\alpha}{\beta} \left(\frac{x}{\beta}\right)^{\alpha-1} e^{-\left(\frac{x}{\beta}\right)^\alpha}, \hspace{0.3cm} x\geq 0
$$

The mean and variance of a random variable with a Weibull distribution can be found by consulting `R` documentation. 

### File Creation Information 

  * File creation date: `r Sys.Date()`
  * Windows version: `r win.version()`
  * `r R.version.string`
  * `mosaic` package version: `r packageVersion("mosaic")`
  * `tidyverse` package version: `r packageVersion("tidyverse")`



