---
title: "Simulation Based Regression Notes"
author:
- Lt Col Ken Horton
- Professor Bradley Warner
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
   - \usepackage{multirow}
   - \usepackage{multicol}
output: 
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align='center')
knitr::opts_chunk$set(out.width = "75%")
library(openintro)
library(knitr)
library(mosaic)
library(tidyverse)
```

\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\Prob}{\mbox{P}}
\newcommand{\diff}{\,\mathrm{d}}


## Objectives

1) Using the bootstrap generate confidence and estimates of standard error for estimates from a linear regression model.

2) Generate and interpret bootstrap confidence and prediction intervals for predicted values.

3) Generate bootstrap samples from sampling rows of the data or sampling residuals. Explain why you might prefer one over the other.

4) Interpret regression coefficients for a linear model with a categorical explanatory variable.

## Introduction

There are at least two ways we can consider creating a bootstrap distribution for a linear model. We can easily
fit a linear model to a resampled data set. But in some situations this may have undesirable features. Influential
observations, for example, will appear duplicated in some resamples and be missing entirely from other resamples.
Another option is to use â€œresidual resampling". In residual resampling, the new data set has all of the predictor
values from the original data set and a new response is created by adding to the fitted function a resampled
residual.

Suppose we have $n$ observations, each with $Y$ and some number of $X$'s, with each observation stored as a row in a data set. The two basic procedures when bootstrapping regression are:
a. bootstrap observations, and  
b. bootstrap residuals.  
The latter is a special case of a more general rule:  
- sample $Y$ from its estimated conditional distribution given $X$.   

In bootstrapping observations, we sample with replacement from the rows of the data; each $Y$ comes with the corresponding $X$'s. In any bootstrap sample some observations may be repeated multiple times, and others not
included.  

In bootstrapping residuals, we fit the regression model, compute predicted values $\hat{Y}_i$ and residuals $e_i = Y_i - \hat{Y}_i$, then create a bootstrap sampling using the same $X$ values as in the original data, but with new $Y$ values obtained using the prediction plus a random residual, $Y^{*}_i = \hat{Y}_i + e^{*}_i$, where
the residuals $e^{*}_i$ are sampled randomly with replacement from the original residuals.   

Bootstrapping residuals corresponds to a designed experiment, where the $x$ values are fixed and only $Y$ is random, and bootstrapping observations to randomly sampled data where both $X$ and $Y$ are sampled. By the principle
of sampling the way the data were drawn, we would bootstrap observations if the $X$'s were random. But we don't have to.

## Confidence intervals for parameters  

To build a confidence interval for the slope parameter, we will resample the data or residuals and generate a new regression model. This process does not assume normality of the residuals. We will use functions from the `mosaic` package to complete this work. However, know that `tidymodels` and `purrr` are more sophisticated tools for doing this work.

### Resampling  

Let's use the Starbucks data again.  

```{r eval=FALSE}
library(openintro)
```

```{r}
star_mod <- lm(calories~carb,data=starbucks)
```

```{r}
star_mod
```


Let's see how `do()` treats a linear model object.

```{r}
obs<-do(1)*star_mod
obs
```

Nice. To resample the data we use `do()` with `resample()`.

```{r}
do(2)*lm(calories~carb,data=resample(starbucks))
```

We are ready to go.

```{r}
set.seed(532)
results <- do(1000)*lm(calories~carb,data=resample(starbucks))
```

```{r}
head(results)
```

With all this data, we can generate confidence intervals for the slope, $R$-squared, and $F$.

```{r}
results %>%
  gf_histogram(~carb) %>%
  gf_vline(xintercept = obs$carb,color="red") %>%
  gf_theme(theme_classic())
```

The confidence interval is found using `cdata()`.

```{r}
cdata(~carb,data=results)
```
We are 95% confident that the true slope is between 3.17 and 5.37. As a reminder, using the normality assumption we had a 95% confidence interval of $(3.21,5.38)$.

```{r}
confint(star_mod)
```
The confidence interval for $R$-squared is

```{r}
cdata(~r.squared,data=results)
```
```{r}
results %>%
  gf_histogram(~r.squared) %>%
  gf_vline(xintercept = obs$r.squared,color="red") %>%
  gf_theme(theme_classic())
```

This is nice.

### Resample residuals

We could also resample the residuals instead of the data. This makes a stronger assumption about the applicability of the linear model. However, it guarantees that every $X$ value is in the resample dataframe.

```{r message=FALSE}
results_resid <- do(1000) * lm( calories~carb, data = resample(star_mod)) # resampled residuals
```

```{r message=FALSE,warning=FALSE}
results_resid %>%
  gf_histogram(~carb) %>%
  gf_vline(xintercept = obs$carb,color="red") %>%
  gf_theme(theme_classic())
```


```{r}
cdata(~carb,data=results_resid)
```

Similar to the previous bootstrap confidence interval just a little narrower.

## Confidence intervals for prediction  

We now want to generate a confidence interval for the average calories from a 60 grams of carbohydrates.  

Using the normal assumption, we had

```{r}
predict(star_mod,newdata = data.frame(carb=60),interval="confidence")
```

We have the slope and intercept in the `results` object. We can use `tidyverse` functions to find the confidence interval.

```{r}
head(results)
```

```{r}
results %>%
  mutate(pred=Intercept+carb*60) %>%
  cdata(~pred,data=.)
```

This is similar to the interval we found last lesson. We are 95% confident that the average calorie content for a menu item with 60 grams of carbohydrates is between 380.8 and 425.7.

### Prediction interval  

The prediction interval is more difficult. We have to account for the variability of the slope but also the residual since this is an individual observation. What we are going to do is sample with replacement from the residuals and then add this value to the predicted value in the last step.

First as a reminder, the prediction interval at 60 grams of `carb`.

```{r}
predict(star_mod,newdata = data.frame(carb=60),interval="prediction")
```
If we are generating a bootstrap of size 1000, we will resample from the residuals 1000 times.

```{r}
results %>%
  mutate(pred=Intercept+carb*60) %>% 
  cbind(resid=sample(star_mod$residuals,size=1000,replace = TRUE)) %>%
  mutate(pred_ind=pred+resid) %>%
  cdata(~pred_ind,data=.)
```

Close, just a little bit different.

## Categorical predictor  

We want to finish up simple linear regression by discussing a categorical predictor. It changes our interpretation somewhat.

Thus far, we have only discussed regression in the context of a quantitative, continuous response AND a quantitative, continuous predictor. We can build linear models with categorical predictor variables as well. 

In the case of a binary covariate, nothing about the linear model changes. The two levels of the binary covariate are typically coded as 1 and 0, and the model is built, evaluated and interpreted in an analogous fashion as before. 

In the case of a categorical covariate with $k$ levels, where $k>2$, we need to include $k-1$ *dummy variables* in the model. Each of these dummy variables takes the value 0 or 1. For example, if a covariate has $k=3$ categories or levels (say A, B or C), we create two dummy variables, $X_1$ and $X_2$, each of which can only take values 1 or 0. If $X_1=1$, the covariate takes the value A. If $x_2=1$, the covariate takes the value B. If both $X_1=0$ and $X_2=0$, this is known as the reference category, and in this case the covariate takes the value C. The arrangement of the levels of the categorical covariate are arbitrary and can be adjusted by the user. This coding is called **contrasts** and again is typically taught in a course on linear models.

The linear model is $Y=\beta_0 + \beta_1X_1 + \beta_2X_2+e$. 

When the covariate takes the value A, $\E(Y)=\beta_0 + \beta_1$. 

When the covariate takes the value B, $\E(Y)=\beta_0 + \beta_2$. 

When the covariate takes the value C, $\E(Y)=\beta_0$. 

Based on this, think about how you would interpret the coefficients $\beta_0$, $\beta_1$, and $\beta_2$. 

### Lending Club 

This data set represents thousands of loans made through the Lending Club platform, which is a platform that allows individuals to lend to other individuals. Of course, not all loans are created equal. Someone who is a essentially a sure bet to pay back a loan will have an easier time getting a loan with a low interest rate than someone who appears to be riskier. And for people who are very risky? They may not even get a loan offer, or they may not have accepted the loan offer due to a high interest rate. It is important to keep that last part in mind, since this data set only represents loans actually made, i.e. do not mistake this data for loan applications! The data set is `loans_full_schema` from the `openintro` package.

```{r eval=FALSE}
library(openintro)
```

```{r}
dim(loans_full_schema)
```

This is a big data set. For educational purposes, we will sample 100 points from the original data. We need to drop the extra factor levels in `homeownership` that have zero observations.

```{r}
tally(~homeownership,data=loans_full_schema,format="proportion")
```
We to sample the data so that each level of home ownership has the same proportion as the original, a stratified sample.

```{r}
loans100 <- loans_full_schema %>%
  select(interest_rate,homeownership) %>%
  droplevels() %>%
  group_by(homeownership) %>%
  slice_sample(prop=0.01) %>%
  ungroup()
```

```{r}
tally(~homeownership,data=loans100,format="proportion")
```

Let's look at the data.

```{r}
loans100 %>%
  gf_boxplot(interest_rate~homeownership) %>%
  gf_theme(theme_classic())
```


It appears that there is some evidence that home ownership impacts the interest rate. We can build a linear model to explore whether this difference in significant. We can use the `lm()` function in `R`, but in order to include a categorical predictor, we need to make sure that variable is stored as a "factor" type. If it is not, we'll need to convert it. 

```{r}
str(loans100)
```

Now we can build the model: 

```{r}
loan_mod<-lm(interest_rate ~ homeownership,data=loans100)
summary(loan_mod)
```

Note that by default, `R` set the `MORTGAGE` level as the reference category. This is because it is first alphabetically. You can control this by changing the order of the factor levels. The package `forcats` helps with this effort. 

How would we interpret this output? Since `MORTGAGE` is reference category, the intercept is effectively the estimated, average, interest rate for home owners with a mortgage. 

```{r}
loans100 %>%
  filter(homeownership == "MORTGAGE") %>%
  summarise(average=mean(interest_rate))
```

The other terms represent the expected difference in delivery times for the other locations. 

```{r}
loans100 %>%
  group_by(homeownership) %>%
  summarise(average=mean(interest_rate),std_dev=sd(interest_rate))
```


Specifically, on average, interest rates for home owners who own the house is 3.193 percent less than those with a mortgage those who rent is 0.8564 percent higher on average. 

>**Exercise**:  
Using the coefficient from the regression model, how do we find the difference in average interest rates between home owners and renters?  

The first coefficient $\beta_{homeownershipOWN} = \mu_{OWN} - \mu_{MORTGAGE}$ and $\beta_{homeownershipRENT} = \mu_{RENT} - \mu_{MORTGAGE}$. Thus $\mu_{OWN} -\mu_{RENT} = \beta_{homeownershipOWN} - \beta_{homeownershipRENT}$, the difference in coefficients.

The model is not fitting a line to the data but just estimating average with the coefficients representing difference from the reference level.

The `Std.Error`, `t value`, and `Pr(>|t|)` values can be used to conduct inference about the respective estimates. It appears that a significant difference in mean interest rates between owners and those with a mortgage exists. 

### Bootstrap

From the boxplots, the biggest difference in means is between home owners and renters. However, in the regression output there is not p-value to test this difference. An easy solution would be to change the reference level but what if you had many levels? How would you know which ones to test? In the next section we will look at multiple comparisons but before then we can use the bootstrap to help us.

Let's bootstrap the regression.

```{r}
set.seed(532)
results <- do(1000)*lm(interest_rate ~ homeownership,data=resample(loans100))
```

```{r}
head(results)
```

We of course can generate a confidence interval on either of the coefficients in the `results` object.

```{r}
obs<-do(1)*loan_mod
obs
```

```{r}
results %>%
  gf_histogram(~homeownershipOWN) %>%
  gf_vline(xintercept = obs$homeownershipOWN,color="red") %>%
  gf_theme(theme_classic())
```


```{r}
cdata(~homeownershipOWN,data=results)
```

Which is similar to the results assuming normality.

```{r}
confint(loan_mod)
```

However, we want a confidence interval for the difference between home owners and renters.

```{r}
results %>%
  mutate(own_rent=homeownershipOWN - homeownershipRENT) %>%
  cdata(~own_rent,data=.)
```

Done! From this interval we can infer that home owners have a significantly lower interest rate than renters.

### ANOVA Table

As a reminder, we could also report the results of loans analysis using an *analysis of variance*, or ANOVA, table. 

```{r}
anova(loan_mod)
```

This table lays out how the variation between observations is broken down. This is a simultaneous test of equal of the three means. Using the $F$-statistic, we would reject the null hypothesis of no differences in mean response across levels of the categorical variable. Notice it is the same p-value reported for the $F$ distribution in the regression summary.

### Pairwise Comparisons

The ANOVA table above (along with the summary of the linear model output before that) merely tells you whether any difference exists in the mean response across the levels of the categorical predictor. It does not tell you where that difference lies. In the case of using regression we can compare `MORTGAGE` to the other two levels but can't conduct a hypothesis of `OWN` vs `RENT`.  In order to make all pairwise comparisons, we need another tool. A common one is the Tukey method. Essentially, the Tukey method conducts three hypothesis tests (each under the null of no difference in mean) but corrects the $p$-values based on the understanding that we are conducting three simultaneous hypothesis tests with the same set of data and we don't want to inflate the Type 1 error. 

We can obtain these pairwise comparisons using the `TukeyHSD()` function in `R`. The "HSD" stands for "Honest Significant Differences". This function requires an `anova` object, which is obtained by using the `aov()` function:
```{r}
TukeyHSD(aov(interest_rate~homeownership, data=loans100))
```

According to this output, only the average interest rate for owners is different from renters. 

## Assumptions

Keep in mind that ANOVA is a special case of a simple linear model. Therefore, all of the assumptions remain the same except for the linearity. The order of the levels is irrelevant and thus a line does not need to go through the three levels. In order to evaluate these assumptions, we would need to obtain the appropriate diagnostic plots: 


```{r}
plot(loan_mod,2)
```

Normality is suspect but we have a large sample size and thus we did not get much of a difference in results from the bootstrap which does not assume normality.

```{r}
plot(loan_mod,3)
```
The assumption of equal variance is also suspect. The variance for the home owners might be less than that for the other two.

```{r}
plot(loan_mod,5)
```
We have three points that might be outliers but they are not too extreme. In general, nothing in this plot is concerning to us.

## File Creation Information 

  * File creation date: `r Sys.Date()`
  * Windows version: `r win.version()`
  * `r R.version.string`
  * `mosaic` package version: `r packageVersion("mosaic")`
  * `tidyverse` package version: `r packageVersion("tidyverse")`
 






